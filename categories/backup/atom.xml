<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: backup | Blog | OpenEnergyMonitor]]></title>
  <link href="https://blog.openenergymonitor.org/categories/backup/atom.xml" rel="self"/>
  <link href="https://blog.openenergymonitor.org/"/>
  <updated>2020-03-12T17:14:18+00:00</updated>
  <id>https://blog.openenergymonitor.org/</id>
  <author>
    <name><![CDATA[Glyn Hudson]]></name>
    <email><![CDATA[support@openenergymonitor.zendesk.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Website Backup]]></title>
    <link href="https://blog.openenergymonitor.org/2013/10/website-backup/"/>
    <updated>2013-10-01T10:47:00+00:00</updated>
    <id>https://blog.openenergymonitor.org/2013/10/website-backup</id>
    <content type="html"><![CDATA[<p>In the interest of open-source I thought I would share the backup setup we have running for the OpenEnergyMonitor website. I’m relatively new to sys-admin tasks and writing bash scripts so please suggest if you think if something could be implemented better.<br /><br />Backing up our Drupal SQL databases which contain the user credentials and all the forum and text content of the website was relatively easy since the disk space they take up is relatively small. A nightly SQL dump then a scheduled secure FTP bash script running as a nightly cronjob on a Raspberry Pi with external hard drive to download the zipped SQL database does the trick. The FTP login credentials are stored away from prying eyes in .netrc file (with chmod 600), two sets of credentials are required and the relevant .netrc file is copied to the home folder when needed.<br /> <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js?lang=css&amp;skin=sunburst"></script> <br />&lt;pre class="prettyprint"&gt;cp netrc/.netrc1 .netrc<br />today=$(date +”%d-%b-%Y”)<br />ftp -vp -z secure $HOST &lt;&lt; EOT<br />ascii<br />cd /LOCATION OF SQL DUMP ON SERVER<br />get $db_name-$today_backup.gz $LOCAL_BACKUP/$db_name-$today_backup.gz<br />bye<br />EOT<br />rm .netrc<br /><br />&lt;/pre&gt;<br /> Backing up the files (images, documents etc) is a bit more of an issue since the ever increasing size of the content mean it’s impractical and would unnecessary load the server and bandwidth to download a full snapshot every night.<br /><br />I found wget has many customisable options. A nightly scheduled bash script running on a Raspberry Pi with an external hard drive with the following wget options looks at files have been created or modified since the last time the command was run and only downloads the changes. Once the initial download is done the command only takes less then a minute to execute and often only downloads a few Mb of data. The option ‘-N’ tells wget only to download new or modified files<br /><br />&lt;pre class="prettyprint"&gt;cp netrc/.netrc2 .netrc<br />wget -m -nv -N -l 0 -P $LOCAL_BACKUP ftp://$HOST/public_html/FILES_LOCATION -o $LOCAL_BACKUP/filelog=$today.txt<br />rm .netrc<br /># This is what the other options do:<br /># -l 0 infinite level of recursive (folder depth)<br /># -m mirror<br /># -N only download new files<br /># -o logfile<br /># -b run in the background<br /># -q turn off logs<br /># -nv non-verbose logs<br />&lt;/pre&gt;This setup seems to be working well. It has a few weak points and limitations that I can think of:<br />&lt;ul&gt;&lt;li&gt;The wget files backup script only downloads new and modified files, it does not mirror the fact that a file could have been deleted on the server, the file would remain in the backup. &lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;The wget script does not keep historical snapshots meaning that if something bad was to happen it would not be possible to rollback to a certain date in history. <i>Update: I have since had recommend to me Rsnapshot which is a backup utility based on Rsync. Rsnapshot looks great and can work over FTPS. My friend <a href="https://twitter.com/spikeheap">Ryan Brooks</a> wrote a good blog post on how to <a href="http://www.ryanbrooks.co.uk//blog/2013/08/22/simple-rsnapshot-backup-over-ftps/">set up Rsnapshot over FTPS</a>. </i>&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Currently the Raspberry Pi only has the one external 1TB hard drive used for backup, ideally this would be two hard drives in a raid array for double safety Backups are only done nightly, this is plenty good enough for us at the moment but might need to be improved in the future. &lt;/li&gt;&lt;/ul&gt;<br />I think it’s amazing that a little £25 Raspberry Pi is powerful enough to handle backup for several websites. the Pi with an external 1TB hard drive connected through a USB hub consumes only 5.7W making it not too bad to leave on 24/7.<br /><br /> One issue that I had initially with the Pi is that the external hard driver would move from /dev/sdb to /dev/sdc therefore loosing it’s mount point. I think this was caused by the HDD momentarily losing power. Switching to using a<a href="http://shop.pimoroni.com/products/pihub"> Pimoroni PiHub</a> to power the setup and mounting the drive by it’s UUID instead of /dev/xxx reference in fstab fixed the problem:  <br /><br />&lt;pre class="prettyprint"&gt;UUID=2921-FCE8 /home/pi/1TB vfat  user,umask=0000   0   0&lt;/pre&gt;<br />I would be interested to hear if you think how the backup could be implemented more efficiently or more securely.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Backing up your raspberrypi emoncms or emoncms.org account]]></title>
    <link href="https://blog.openenergymonitor.org/2013/09/backing-up-your-raspberrypi-emoncms-or/"/>
    <updated>2013-09-30T20:00:00+00:00</updated>
    <id>https://blog.openenergymonitor.org/2013/09/backing-up-your-raspberrypi-emoncms-or</id>
    <content type="html"><![CDATA[<div class="separator" style="clear: both; text-align: left;">I've added a couple of scripts to the emoncms usefulscripts directory that makes backing up an emoncms account whether on a local raspberrypi or emoncms.org much easier than the sync module solution, although still not as easy as I would like it to be, I would like setting up a backup to be as easy as installing a dropbox client.&nbsp;</div>
<div class="separator" style="clear: both; text-align: left;"><br /></div>
<div class="separator" style="clear: both; text-align: left;">The scripts work with Linux at the moment, hopefully soon I'l get a solution running on Windows. I would highly recommend keeping your own backup of your data, a backup of your emoncms data on your main computer can also be useful for both quicker access to the data and doing additional analysis of the data.</div>
<div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-MIl8j4hmkjA/UknQBGmTKMI/AAAAAAAACxE/QOk0JRh5EKQ/s1600/backup.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="238" src="http://3.bp.blogspot.com/-MIl8j4hmkjA/UknQBGmTKMI/AAAAAAAACxE/QOk0JRh5EKQ/s640/backup.png" width="640" /></a></div>
<p><b>1) Install emoncms</b> <b>on your backup machine</b> following the guide here: <a href="http://emoncms.org/site/docs/installlinux">http://emoncms.org/site/docs/installlinux</a><br />Create an account and note down your mysql credentials.<br /><br /><b>2) Download the usefulscripts repository</b><br /><a href="https://github.com/emoncms/usefulscripts">https://github.com/emoncms/usefulscripts</a><br /><br />There are two scripts available under usefulscripts/replication<br /><br />&lt;ul&gt;&lt;li&gt;import_full.php&lt;/li&gt;&lt;li&gt;import_inputs.php&lt;/li&gt;&lt;/ul&gt;<br />3) Try<b> importing your inputs</b> first to test that it works: Open to edit <i>import_inputs.php</i>.<br />Set your mysql database name, username and password, the same credentials as for the settings.php step of the emoncms installation. Set the $server variable to the location of the raspberrypi or emoncms.org account you want to backup and set the $apikey to the write apikey of that account.<br /><br />In terminal, goto the usefulscripts/replication directory. Run import_inputs.php:<br /><br />    php import_inputs.php<br /><br />If successful you should now see your input list and all input processing information backed up in your local emoncms account.<br /><br /><b>4) Backup all your feed data:</b><br /><br />As for importing inputs open import_full.php and set the database credentials and remote emoncms account details.<br /><br />Run the backup script with <b>sudo</b><br /><b><br /></b>    sudo php import_full.php<br /><br />That’s it, it should now work through all your feeds whether mysql or timestore (no phptimeseries support yet) making a local backup. When you first run this script it can take a long time. When you run this script again it will only download the most recent data and so will complete much faster. I run this script once every few days to keep an up-to-date backup.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A new backup system for emoncms.org]]></title>
    <link href="https://blog.openenergymonitor.org/2013/07/a-new-backup-system-for-emoncmsorg/"/>
    <updated>2013-07-09T22:42:00+00:00</updated>
    <id>https://blog.openenergymonitor.org/2013/07/a-new-backup-system-for-emoncmsorg</id>
    <content type="html"><![CDATA[<p>The current emoncms.org backup system works by using two separate servers with data being synced from the main one to the second backup server using the same implementation used in the emoncms sync module <a href="http://openenergymonitor.blogspot.co.uk/2013/05/emoncmsorg-backup.html">http://openenergymonitor.blogspot.co.uk/2013/05/emoncmsorg-backup.html</a><br />&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;A couple of weeks after getting all that setup BigV announced a new feature: archive storage which they say is ideal for backups. Archive storage works in much the same way as connecting a second external drive to a computer. Archive storage on bigv is guaranteed to be on a separate storage pool from the main vm disc which is good news as it wasn’t guaranteed that the two separate server method used seperate storage pools (If I understand correctly)&lt;/div&gt;&lt;div&gt;<a href="http://blog.bytemark.co.uk/2013/06/13/archive-storage-and-web-based-vm-manager-launched">http://blog.bytemark.co.uk/2013/06/13/archive-storage-and-web-based-vm-manager-launched</a>&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;Another advantage to the new archive storage is that its much cheaper to run, costing £2/month for 50GB rather than £16/month for another vm and 30GB extra space.&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;A simple php script is used to perform the backup, it makes use of the direct file access stuff I recently learnt about (<a href="http://openenergymonitor.blogspot.co.uk/2013/07/more-direct-file-storage-research.html">http://openenergymonitor.blogspot.co.uk/2013/07/more-direct-file-storage-research.html</a>) to do incremental backup file copy using php file access commands making it potentially very fast, it can backup files at full file copy speeds:&lt;/div&gt;&lt;div&gt;<a href="https://github.com/emoncms/usefulscripts/blob/master/backup_method2.php">https://github.com/emoncms/usefulscripts/blob/master/backup_method2.php</a>&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;It accesses the mysql data directly, copying the content of the mysql feed data file i.e:  /var/lib/mysql/emoncms/feed_1.MYD to the backup drive.&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;On another slightly related note the data in feed_1.MYD is stored simply as:&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;1 byte for null flag&lt;/div&gt;&lt;div&gt;4 bytes for the timestamp&lt;/div&gt;&lt;div&gt;4 bytes for the float data value<br />repeated…&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;and because the readings are inserted one after the other in ascending time we can actually use the mysql feed data files directly with the direct file get_feed_data method to get 10-20x query speed improvement for generating visualisations: <a href="https://github.com/emoncms/experimental/blob/master/storage/directfiles/get_feed_data.php">https://github.com/emoncms/experimental/blob/master/storage/directfiles/get_feed_data.php</a>&lt;/div&gt;&lt;div&gt;<br />&lt;/div&gt;&lt;div&gt;We could even get rid of the mysql feed table index’s to save disk space although that will probably slow down mysql updates (I need to look into it). This could be a short term measure before a full timestore emoncms implementation is complete which provides many other benefits.&lt;/div&gt;</p>
]]></content>
  </entry>
  
</feed>
